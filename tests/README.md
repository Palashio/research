# Research Report Evaluation System

This folder contains the evaluation system for research reports generated by the automated research pipeline.

## Structure

- `reports/` - Directory containing research reports to evaluate
- `evaluator.py` - Main evaluation class that analyzes report quality
- `run_eval.py` - Interactive script to select and evaluate reports
- `evaluation_*.json` - Individual evaluation results
- `batch_evaluation_summary_*.json` - Batch evaluation summaries

## Usage

### From the root directory:
```bash
python run_evaluation.py
```

### From the tests directory:
```bash
python run_eval.py
```

## Evaluation Criteria

The system evaluates research reports on **Narrative Coherence** using the following criteria:

1. **Logical Flow (1-5)**: How well content flows from one paragraph to the next
2. **Integration Quality (1-5)**: How well information from multiple sources is integrated
3. **Academic Tone (1-5)**: How professional and scholarly the writing style is
4. **Clarity (1-5)**: How clear and understandable the content is
5. **Coherence (1-5)**: How well ideas connect and build upon each other

Scores are converted from 1-5 scale to 0-100 scale for reporting.

## Output

Evaluations are saved as JSON files with detailed scores and explanations for each section, plus an overall score. 